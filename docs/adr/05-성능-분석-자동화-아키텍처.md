# 성능 분석 자동화 아키텍처

Date: 2025-10-10  
Status: Accepted

## 맥락

[이전 문서](/docs/adr/04-performance-analysis-plan)에서 기획한 내용을 구현합니다.  
구체적인 사용 방법, 역할, 효과 등에 대해 기술하고자 합니다.

#### 실행 방법

터미널에서 아래와 같이 실행할 수 있습니다.

```bash
npm run benchmark   # 측정 및 저장
cd analysis
python main.py      # 분석, 시각화, 문서화 자동화
```

#### 측정 방법

- **시나리오:**
<div align="center">

| 시나리오명       | 노드 수 | 엣지 수 | 그룹 수 | 목적            |
| ---------------- | ------- | ------- | ------- | --------------- |
| Small            | 12      | 18      | 2       | 빠른 피드백     |
| Medium           | 60      | 90      | 3       | 일반적인 케이스 |
| Large (Standard) | 120     | 180     | 4       | 기본 기준       |

</div>

- **시행 횟수:**
  - `5 (시드) x 3 (시나리오) x 3 (전략)`
  - 현재 5개의 시드에 대해, 각 전략(`A-Star`, `Bus`, `Vertices`)마다, 3가지 시나리오(`Small`, `Medium`, `Large`)만큼 테스트가 시행됩니다.
  - `scripts/benchmark.config.ts`에서 시드를 설정할 수 있습니다.
  - 동일한 시드는 항상 동일한 그래프를 생성합니다.
  - 예시:
  ```js
  export const SEEDS = [42, 101, 256, 888, 1337];
  ```
  > 향후 필요한 만큼 시드와 시나리오를 늘릴 수 있습니다.  
  > 측정 환경의 한계로, 현재 간단한 측정부터 시작하고자 합니다.
<br/>

## 결정

두 개의 새로운 디렉터리를 도입하여 각각의 역할을 명확히 분리했습니다.

- `scripts/` : 측정을 담당합니다.
- `analysis/` : 분석, 시각화, 문서화 자동화를 담당합니다.

<br/>

### 1. 자동화된 벤치마크 실행 (`scripts/`)

#### 목표:

- 일관된 조건 하에서 각 라우팅 전략의 성능을 측정하고 원시 데이터(raw data)를 수집합니다.

#### 실행:

- 터미널에 `npm run benchmark` 명령어를 입력하여 `scripts/run-benchmark.ts` 파일을 실행합니다.

#### 주요 기능:

- **시나리오 및 시드 기반 테스트:**  
  `scripts/benchmark.config.ts` 에 사전 정의된 시나리오(Small, Medium, Large)와 시드(seed) 값 조합을 순회하며, 동일한 조건의 그래프를 생성하여 테스트의 일관성을 보장합니다.

- **프로파일링:**  
  `scripts/profiler.ts` 의 `Profiler` 클래스를 사용해 각 알고리즘 파이프라인의 단계별 실행 시간(`Placement`, `Routing`, `Post-Process`)과 `Routing` 내부의 세부 실행 시간까지 정밀하게 측정합니다.

- **결과 저장:**  
  측정된 모든 결과는 `analysis/results/YYYY-MM-DD_HH-MM-SS/raw_results.json` 경로에 타임스탬프 기반의 고유한 디렉터리 내에 JSON 파일로 자동 저장됩니다.

<br/>

### 2. 분석, 시각화, 문서화 자동화 (`analysis/`)

#### 목표

- `raw_results.json` 파일을 입력으로 받아, 통계 분석, 데이터 시각화, 최종 리포트 문서화를 자동으로 수행합니다.

#### 실행

- `analysis` 디렉터리 내에서 `main.py` 파일을 실행합니다.

```bash
cd analysis
python main.py
```

#### 디렉터리 구조

```plaintext
analysis/
├─ results/
│  └─ [YYYY-MM-DD_HH-MM-SS/]
│     ├─ charts/              # 차트 이미지
│     ├─ raw_results.json     # 측정 데이터
│     ├─ summary.json         # 통계 데이터
│     └─ report_frame.md      # 생성된 리포트
├─ data_loader.py          # 데이터 로딩
├─ analyzer.py             # 데이터 분석
├─ visualizer.py           # 차트 생성
├─ report_generator.py     # 리포트 생성
└─ main.py                 # 전체 파이프라인 실행
```

#### 주요 기능

- **데이터 로딩 및 전처리:**  
  `data_loader.py` 가 가장 최근에 실행된 벤치마크 결과(`raw_results.json`)를 자동으로 찾아 `Pandas DataFrame(df)`으로 변환합니다.

- **통계 분석:**  
  `analyzer.py` 가 시나리오 및 전략별 성능(평균, 표준편차, 최소/최대값)을 계산하고 `summary.json` 파일로 저장합니다.

- **시각화:**  
  visualizer.py 가 분석된 데이터를 기반으로 전체 성능 비교 막대 차트, 단계별 성능 분석 파이 차트 등 다양한 시각 자료를 생성하여 `charts/` 디렉터리에 저장합니다.

- **리포트 자동 생성:**  
  `report_generator.py` 가 통계 데이터와 시각화 차트를 종합하여 `report_frame.md` 형식의 최종 결과 보고서 형식을 생성합니다.
  > 결과 분석은 직접 작성해야 합니다.

## 결과

#### 재현성 및 신뢰성 확보

- Seed 기반 테스트 환경과 자동화된 파이프라인을 통해, 언제든 동일한 조건에서 성능을 측정하고 비교할 수 있게 되었습니다.

#### 신속한 분석 및 의사결정

- `npm run benchmark` 와 `python main.py` 두 명령어만으로 전체 측정부터 리포트 생성까지 완료되어, 알고리즘 개선 후 성능 변화를 즉각적으로 파악하고 다음 개선 방향을 빠르게 결정할 수 있습니다.

#### 데이터 기반 최적화

- 정량적인 데이터와 시각화된 차트를 통해 병목 지점을 명확히 식별하고, 개선 효과를 객관적인 수치로 검증할 수 있는 체계적인 워크플로우를 구축했습니다.

#### 문서화 자동화

- 모든 실험 결과가 타임스탬프 기반의 개별 폴더에 리포트와 함께 자동으로 기록되어, 과거 실험 내용과 결과를 추적하고 관리하기 용이해졌습니다.
